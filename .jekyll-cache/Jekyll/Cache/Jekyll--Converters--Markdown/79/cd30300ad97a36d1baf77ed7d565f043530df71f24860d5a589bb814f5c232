I"è&<div class="alert alert-primary" role="alert">
If you need <strong>GPU</strong>, Data, or Computational resources to
complete your science, contact the OSG facilitation
team at <a href="mailto:help@opensciencegrid.org">help@opensciencegrid.org</a>
to arrange a meeting. OSG provides free, opportunistic high throughput
computing resources to the U.S. research community.
</div>

<p>The San Diego Supercomputer Center (SDSC) and the
Wisconsin IceCube Particle Astrophysics Center (WIPAC) at the University
of Wisconsin‚ÄìMadison successfully completed a computational experiment
as part of a multi-institution collaboration that marshalled all
globally available for sale GPUs (graphics processing units) across
Amazon Web Services, Microsoft Azure, and the Google Cloud
Platform.</p>

<p>In all, some 51,500 GPU processors were used during the
approximately 2-hour experiment conducted on November 16 and funded
under a <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1941481&amp;HistoricalAwards=false">National Science Foundation EAGER
grant</a>.
The experiment used simulations from the <a href="https://icecube.wisc.edu/">IceCube Neutrino
Observatory</a>, an array of some 5,160
optical sensors deep within a cubic kilometer of ice at the South Pole.
In 2017, researchers at the NSF-funded observatory <a href="https://www.sdsc.edu/News%20Items/PR20180712_IceCube_neutrino.html">found the first evidence of a source of high-energy
cosmic
neutrinos</a>
‚Äì subatomic particles that can emerge from their sources and pass
through the universe unscathed, traveling for billions of light years to
Earth from some of the most extreme environments in the universe.</p>

<figure class="figure">
<div class="container">
    <div class="row">
        <div class="col-md">
            <img src="https://raw.githubusercontent.com/CHTC/Articles/main/images/numgpus.png" class="img-fluid figure-img" />
        </div>
        <div class="col-md">
            <img src="https://raw.githubusercontent.com/CHTC/Articles/main/images/gpupflops.png" class="img-fluid figure-img" />
        </div>
    </div>
</div>
<figcaption class="figure-caption">Number and PFLOPS32 provided to IceCube Computing</figcaption>
</figure>

<p>The experiment ‚Äì completed just prior to the opening of
the International Conference for High Performance Computing, Networking,
Storage, and Analysis (SC19) in Denver, CO ‚Äì was coordinated by Frank
W√ºrthwein, SDSC Lead for High-Throughput Computing, and Benedikt Riedel,
Computing Manager for the IceCube Neutrino Observatory and Global
Computing Coordinator at <a href="https://wipac.wisc.edu/">WIPAC</a>.</p>

<p>Igor Sfiligoi, SDSC‚Äôs lead scientific software developer
for high-throughput computing, and David Schultz, a production software
manager with IceCube, conducted the actual run.</p>

<p>‚ÄúWe focused this GPU cloud burst in the area of multi-messenger astrophysics, which is based on the
observation and analysis of what we call ‚Äòmessenger‚Äô signals, in this
case neutrinos,‚Äù said W√ºrthwein, also a physics professor at the
University of California San Diego and Executive Director of the <a href="https://opensciencegrid.org/">OSG</a>, a multi-disciplinary
research partnership specializing in high-throughput computational
services funded by the NSF.</p>

<p>‚ÄúThe NSF chose multi messenger astronomy as one of its
<a href="https://www.nsf.gov/news/special_reports/big_ideas/universe.jsp">10 Big
Ideas</a>
to focus on during the next few years,‚Äù said W√ºrthwein. ‚ÄúWe now have
instruments that can measure gravitational waves, neutrinos, and various
forms of light to see the most violent events in the universe. We‚Äôre
only starting to understand the physics behind such energetic celestial
phenomena that can reach Earth from deepest space.‚Äù</p>

<p>The net result was a peak of about 51k GPUs of various
kinds, with an aggregate peak of about 380 PFLOP32s (according to NVIDIA
specifications), according to Sfiligoi.</p>

<figure class="figure ">
<table class="table table-responsive">
    <thead>
    <tr>
      <td><strong>GPU Specs</strong></td>
      <td>V100</td>
      <td>P100</td>
      <td>P40</td>
      <td>P4</td>
      <td>T4</td>
      <td>M60</td>
      <td>K80</td>
      <td>K520</td>
    </tr>
    </thead>
    <tbody>
    <tr>
      <td>Num GPUS</td>
      <td>9.2k</td>
      <td>7.2k</td>
      <td>2.1k</td>
      <td>0.5k</td>
      <td>4.6k</td>
      <td>10.1k</td>
      <td>12.5k</td>
      <td>5.4k</td>
    </tr>
    <tr>
      <td>PFLOP32s</td>
      <td>132.2</td>
      <td>68.1</td>
      <td>25.2</td>
      <td>2.5</td>
      <td>38.6</td>
      <td>48.8</td>
      <td>51.6</td>
      <td>12.4</td>
    </tr>
  </tbody>
</table>
<figcaption class="figure-caption">Absolute number of resources provided to IceCube</figcaption>
</figure>

<p>‚ÄúFor comparison, the Number 1 TOP100 HPC system, <a href="https://www.olcf.ornl.gov/summit/"><em>Summit</em>, (based at Oak Ridge National
Laboratory)</a> has a nominal
performance of about 400 PFLOP32s. So, at peak, our cloud-based cluster
provided almost 95% of the performance of <em>Summit</em>, at least for the
purpose of IceCube simulations.</p>

<p>The relatively short time span of the experiment showed
the ability to conduct a massive amount of data processing within a very
short period ‚Äì an advantage for research projects that must meet a tight
deadline. Francis Halzen, principal investigator for IceCube, a
Distinguished Professor at the University of Wisconsin‚ÄìMadison, and
director of the university‚Äôs Institute for Elementary Particle Physics,
foresaw this several years ago.</p>

<p>‚ÄúWe have initiated an effort to improve the calibration
of the instrument that will result in sensitivity improved by an
estimated factor of four,‚Äù wrote Halzen. ‚ÄúWe can apply this improvement
to 10 years of archived data, thus obtaining the equivalent of 40 years
of current IceCube data.‚Äù</p>

<p>‚ÄúWe conducted this experiment with three goals in mind,‚Äù
said IceCube‚Äôs Riedel. ‚ÄúOne obvious goal was to produce simulations that
will be used to do science with IceCube for multi-messenger
astrophysics. But we also wanted to understand the readiness of our
cyberinfrastructure for bursting into future Exascale-class facilities
such as Argonne‚Äôs <em>Aurora</em> or Oak Ridge‚Äôs <em>Frontier,</em> when they become
available. And more generally, we sought to determine how much GPU
capacity can be bought today for an hour or so GPU burst in the
commercial cloud.‚Äù</p>

<p>‚ÄúThis was a social experiment as well,‚Äù added W√ºrthwein.
‚ÄúWe scavenged up all available GPUs on demand across 28 cloud regions
across three continents ‚Äì North America, Europe, and Asia. The results
of this experiment tell us that we can elastically burst to very large
scales of GPUs using the cloud, given that exascale computers don‚Äôt
exist now but may soon be used in the coming years. The demo also shows
such bursting of massive data, is suitable for a wide range of
challenges across astronomy and other sciences. To the extent that the
elasticity is there, we believe that this can be applied across all of
scientific research to get results quickly.‚Äù</p>

<figure class="figure float-left">
<img src="https://raw.githubusercontent.com/CHTC/Articles/main/images/byregion.png" class="img-fluid figure-img" width="525" />
<figcaption class="figure-caption">Regions used in the GPU experiment across AWS, GCP, and Azure</figcaption>
</figure>

<p>HTCondor was used to integrate all purchased GPUs into a
single resource pool to which IceCube submitted their workflows from
their home base in Wisconsin. This was accomplished by aggregating
resources in each cloud region, and then aggregating those aggregators
into a single global pool at SDSC.</p>

<p>‚ÄúThis is very similar to the production infrastructure
that OSG operates for IceCube to aggregate dozens of ‚Äòon-prem‚Äô clusters
into a single global resource pool across the U.S., Canada, and Europe,‚Äù
said Sfiligoi.</p>

<p>An additional experiment to reach even higher scales is
likely to be made sometime around the Christmas and New Year holidays,
when commercial GPU use is traditionally lower, and therefore
availability of such GPUs for scientific research is greater.</p>

<p>Acknowledgment: Thanks to the NSF for their support of
this endeavor as part of the
<a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1941481&amp;HistoricalAwards=false">OAC-1941481</a>,
<a href="https://nsf.gov/awardsearch/showAward?AWD_ID=1148698&amp;HistoricalAwards=false">MPS-1148698</a>,
<a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1841530&amp;HistoricalAwards=false">OAC-1841530</a>
and
<a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1826967&amp;HistoricalAwards=false">OAC-1826967</a>.
Special thanks also to all the support personnel from AWS, Azure, Google
Cloud and Strategic Blue, who helped raise all the necessary quotas and
limits. And all of this would of course not be possible without the hard
work of Igor Sfiligoi, David Schultz, Frank W√ºrthwein and Benedikt Riedel.</p>

<h3 id="related-articles">Related Articles</h3>
<ul>
  <li><a href="https://www.hpcwire.com/2019/11/22/51000-cloud-gpus-converge-to-power-neutrino-discovery-at-the-south-pole/">HPCWire - 51,000 Cloud GPUs Converge to Power Neutrino Discovery at the South Pole</a></li>
  <li><a href="https://insidehpc.com/2019/11/sdsc-conducts-50000-gpu-cloudburst-experiment-with-wisconsin-icecube-particle-astrophysics-center/">Inside HPC - SDSC Conducts 50,000+ GPU Cloudburst Experiment with Wisconsin IceCube Particle Astrophysics Center</a></li>
  <li><a href="https://www.linkedin.com/pulse/using-50k-gpus-across-multiple-clouds-icecube-science-igor-sfiligoi/">Igor Sfiligoi - Using 50k GPUs across multiple Clouds for IceCube Science</a></li>
</ul>

:ET